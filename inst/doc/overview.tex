
\MatchIt\ is designed for studies with a dependent variable (or a set
of dependent variables) that is a function of a dichotomous causal (or
``treatment'') variable, with values known as the ``treated'' and
``control'' groups, and a set of ``pretreatment'' covariates, i.e.,
that are causally prior to the administration of the treatment.  (If
you are interested in the causal effect of more than one variable in
your data set, run \MatchIt\ separately for each one; it is unlikely
in any event that any one parametric model will produce valid causal
inferences for more than one treatment variable at a time.)  \MatchIt\
can be used for other types of causal variables by dichotomizing them,
perhaps in multiple ways \citep[see also][]{ImaDyk04}.  \MatchIt\
works for experimental data, but is designed mainly for observational
studies where the treatment variable is simply observed rather than
manipulated at will by the investigator.

We adopt the same notation as in \citet*{HoImaKin06}. Unless otherwise
noted, let $i$ index the $n$ units in the dataset, $n_1$ denote the
number of treated units, $n_0$ denote the number of control units
(such that $n=n_0+n_1$), and $x_i$ indicate a vector of pretreatment
(or control) variables for unit $i$.  Let $t_i=1$ when unit $i$ is
assigned treatment, and $t_i=0$ when unit $i$ is assigned control.
(The labels ``treatment'' and ``control'' are arbitrary and can be
switched for convenience.)  Denote $y_i(1)$ as the potential outcome
of unit $i$ under treatment --- the value the outcome variable would
take if $t_i$ were equal to 1, whether or not $t_i$ in fact is 0 or 1
-- and $y_i(0)$ the potential outcome of unit $i$ under control ---
the value the outcome variable would take if $t_i$ were equal to 0,
regardless of its value in fact.  The variables $y_i(1)$ and $y_i(0)$
are jointly unobservable, and for each $i$, we observe one
$y_i=t_iy_i(1)+(1-t_i)y_i(0)$, and not the other.

\section{Preprocessing via Matching}

If $t_i$ and $X_i$ were independent, we would not need to control for
$X_i$, and any parametric analysis would effectively reduce to a
difference in means of $Y$ for the treated and control groups.  The
goal of matching is to preprocess the data prior to the parametric
analysis so that the actual relationship between $t_i$ and $X_i$ is
eliminated or reduced without introducing bias and inefficiency.  When
matching we select, duplicate, or selectively drop observations from
our data, and we do so without inducing bias as long as we use a rule
that is a function only of $t_i$ and $X_i$ and does not depend on the
outcome variable $Y_i$.  \MatchIt\ implements and evaluates the choice
of these rules.  

The simplest way to obtain good matches (as defined above) is to use
one-to-one exact matching, which pairs each treated unit with one
control unit for which the values of $X_i$ are identical.  However,
with many covariates and finite numbers of potential matches, it is
often very difficult to obtain exact matches.  Fortunately, good
matching only requires that the empirical \emph{distribution} of $X$
given $t=0$ match that of $X$ given $t=1$, and so individual (exactly)
matched pairs are not required.  Formally the goal of matching is
$\tilde p(X\mid t=1) \approx \tilde p(X\mid t=0)$, where $\tilde p$
refers to the observed empirical density of the data (think
histogram), rather than a population density.  Indeed, many of the
other methods implemented in \MatchIt\ only attempt to balance the
overall covariate distributions, without necessarily finding
one-to-one exact matches.

A key point in \citet*{HoImaKin06} is that matching methods by
themselves are not methods of estimation: Every use of matching in the
literature involves an analysis step following the matching procedure,
but almost all analyses use a simple difference in means.  This
procedure is appropriate only if exact matching was conducted.  In
almost all other cases, some adjustment is required, and there is no
reason to degrade your inferences by using an inferior method of
analysis such as a difference in means even when improving your
inferences via preprocessing.  Thus, with \MatchIt, you can improve
your analyses in two ways.  In fact, \MatchIt\ analyses are ``doubly
robust'' in that if \emph{either} the matching analysis or the
analysis model is correct (but not necessarily both) your inferences
will be statistically consistent.

\section{Checking Balance}
\label{sec:balance-sum}

The goal of matching is to create a data set that looks closer to one
that would result from a randomized experiment.  When we get close, we
break the link between treatment variable and the pretreatment
controls, which makes the parametric form of the analysis model less
relevant or irrelevant entirely.  To break this link, we need the
distribution of covariates to be the same within the matched treated
and control groups.

A crucial part of any matching procedure is, therefore, to assess how
close the (empirical) covariate distributions are in the two groups,
which is known as ``balance.''  Because the outcome variable is not
used in the matching procedure, a variety of matching methods can be
assessed, and the matching procedure that leads to the best balance is
chosen.  \MatchIt\ provides a number of ways to assess the balance of
covariates after matching, including numerical summaries such as the
bias (difference in means) or standardized bias (difference in means
divided by the treated group standard deviation), and graphical
summaries such as quantile-quantile plots that compare the empirical
distributions of each covariate, and numerical summaries of this
graphical summary.  The widely used procedure of doing t-tests of the
difference in means is highly misleading and should never be used to
assess balance.  These diagnostics can be done on all the covariates
that are included in the matching procedure, as well as on other
covariates on which close matches are desired.

\section{Conducting Analyses after Matching}

The most common way that parametric analyses are used to compute
quantities of interest (without matching) is by holding constant some
explanatory variables, changing others, and computing predicted or
expected values and taking the difference or ratio, all by using the
parametric functional form.  In the case of causal inference, this
would mean looking at the effect on the expected value of the outcome
variable when changing $T$ from 0 to 1, while holding constant the
pretreatment control variables $X$ at their means or medians.  This,
and indeed any other standard procedure, would be a perfectly
reasonable way to proceed with analysis after matching.

Another increasingly popular way to proceed with analysis after
\MatchIt\ is to compute the \emph{average treatment effect on the
  treated}.  For example, for the treated group, the potential
outcomes under control, $Y_i(0)$, are missing, whereas the outcomes
under treatment, $Y_i(1)$, are observed, and the goal of the analysis
is to impute the missing outcomes, $Y_i(0)$ in observations where
$T_i=1$.  We do this via simulation using a parametric statistical
model (as described below).  Once those potential outcomes are imputed
from the model, the estimate of individual $i$'s treatment effect is
$Y_i(1)-\widehat{Y}_i(0)$ where $\widehat{Y}_i(0)$ is a Monte Carlo
estimate of the average missing potential outcome for unit $i$ (i.e.,
the average of simulated values of the dependent variable for unit $i$
under the counterfactual condition where $T_i=0$).  The in-sample
average treatment effect for the treated individuals can then be
obtained by averaging this difference over all observations $i$ where
in fact $T_i=1$.  (A similar procedure can also be used to estimate
various other quantities of interest such as the average treatment
effect for all observations.)  An advantage of this simulation
approach is that the uncertainty estimates such as standard errors and
confidence intervals are obtained easily by the usual rules in fitting
the parametric model.

The imputation from the model can be done in at least two ways.
Recall that the model is used to impute \emph{the value that the
  outcome variable would take among the treated units if those treated
  units were actually controls}.  Thus, one reasonable approach would
be to fit a model to the matched data and create simulated predicted
values of the dependent variable for the treated units with $T_i$
switched counterfactually from 1 to 0.  An alternative approach would
be to fit a model without $T$ by using only the outcomes of the
matched control units (i.e., using only observations where $T_i=0$).
Then, given this fitted model, the missing outcomes $Y_i(0)$ are
imputed for the matched treated units by using the values of the
explanatory variables for the treated units.  The first approach will
usually have lower variance, since all observations are used, and the
second may have less bias, since no assumption of constant parameters
is needed.  See \citet*{HoImaKin06} for more details.

% Local Variables: 
%%% mode: latex
%%% TeX-master: "matchit"
%%% End: 
